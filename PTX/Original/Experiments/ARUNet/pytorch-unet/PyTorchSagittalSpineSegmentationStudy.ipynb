{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import sample\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import girder_client\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_unet import UNet\n",
    "from dataset_loader import DataAugmentor\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save timestamp: 2021-08-05_19-38-26\n"
     ]
    }
   ],
   "source": [
    "this_notebook_name = \"PyTorchSagittalSpineSegmentationStudy\"\n",
    "\n",
    "# Update this folder name for your computer\n",
    "\n",
    "local_data_folder = r\"/home/nick/dev/SaggitalSpineSegmentation_Data\"\n",
    "overwrite_existing_data_files = False\n",
    "\n",
    "# All results and output will be archived with this timestamp\n",
    "\n",
    "save_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(\"Save timestamp: {}\".format(save_timestamp))\n",
    "\n",
    "# Learning parameters\n",
    "ultrasound_size = 128\n",
    "num_classes = 2\n",
    "num_epochs = 500 # was 500\n",
    "batch_size = 128\n",
    "max_learning_rate = 0.025\n",
    "min_learning_rate = 0.00005\n",
    "\n",
    "# I will use exponential learning rate decay, not linear\n",
    "# need to solve the system of equations:\n",
    "\n",
    "# x**y = max_learning_rate\n",
    "# x**(num_epochs + y) = min_learning_rate\n",
    "\n",
    "# Here, x is the decay factor we want\n",
    "# solving analytically by hand bc I am a math major (and sympy failed):\n",
    "\n",
    "# y*ln(x) = ln(max_learning_rate)\n",
    "# (num_epochs + y) * ln(x) = ln(min_learning_rate)\n",
    "# (num_epochs + (ln(max_learning_rate)/ln(x)))*ln(x) = ln(min_learning_rate)\n",
    "# ln(x)*num_epochs + ln(max_learning_rate) = ln(min_learning_rate)\n",
    "# ln(x) = (ln(min_learning_rate) - ln(max_learning_rate))/num_epochs\n",
    "# ln(x) = ln( (min_learning_rate / max_learning_rate)**(1/num_epochs) )\n",
    "# x = (min_learning_rate / max_learning_rate)**(1/num_epochs)\n",
    "\n",
    "learning_rate_decay = (min_learning_rate / max_learning_rate)**(1/num_epochs)\n",
    "\n",
    "regularization_rate = 0.001\n",
    "filter_multiplier = 10\n",
    "WCE_weights = np.array([0.1, 0.9])\n",
    "\n",
    "# Training data augmentation parameters\n",
    "\n",
    "max_shift_factor = 0.12\n",
    "max_rotation_angle = 10\n",
    "max_zoom_factor = 1.1\n",
    "min_zoom_factor = 0.8\n",
    "\n",
    "# Evaluation parameters\n",
    "\n",
    "acceptable_margin_mm = 1.0\n",
    "mm_per_pixel = 1.0\n",
    "\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001]\n",
    "\n",
    "limit_validation_rounds = -1\n",
    "\n",
    "# Uncomment for faster debugging\n",
    "\n",
    "# roc_thresholds = [0.8, 0.6, 0.4, 0.2, 0.1, 0.01]\n",
    "# limit_validation_rounds = 1\n",
    "# num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what data to download\n",
    "\n",
    "girder_api_url = \"https://pocus.cs.queensu.ca/api/v1\"\n",
    "\n",
    "training_ultrasound_ids = [\n",
    "    \"5da9e5c0d9e6a3be02d012b4\",\n",
    "    \"5da9e5c7d9e6a3be02d012c6\",\n",
    "    \"5da9e5c2d9e6a3be02d012b7\",\n",
    "    \"5da9e5c3d9e6a3be02d012ba\",\n",
    "    \"5da9e5c8d9e6a3be02d012c9\",\n",
    "    \"5da9e5c5d9e6a3be02d012c0\",\n",
    "    \"5da9e5c6d9e6a3be02d012c3\",\n",
    "    \"5da9e5c4d9e6a3be02d012bd\"\n",
    "]\n",
    "\n",
    "training_ultrasound_filenames = [\n",
    "    \"q000_ultrasound.npy\",\n",
    "    \"q001_ultrasound.npy\",\n",
    "    \"q002_ultrasound.npy\",\n",
    "    \"q003_ultrasound.npy\",\n",
    "    \"q004_ultrasound.npy\",\n",
    "    \"q005_ultrasound.npy\",\n",
    "    \"q006_ultrasound.npy\",\n",
    "    \"q007_ultrasound.npy\"\n",
    "]\n",
    "\n",
    "training_segmentation_ids = [\n",
    "    \"5da9e5c8d9e6a3be02d012cc\",\n",
    "    \"5da9e5ccd9e6a3be02d012de\",\n",
    "    \"5da9e5c9d9e6a3be02d012cf\",\n",
    "    \"5da9e5cad9e6a3be02d012d2\",\n",
    "    \"5da9e5cdd9e6a3be02d012e1\",\n",
    "    \"5da9e5cbd9e6a3be02d012d8\",\n",
    "    \"5da9e5cbd9e6a3be02d012db\",\n",
    "    \"5da9e5cad9e6a3be02d012d5\"\n",
    "]\n",
    "\n",
    "training_segmentation_filenames = [\n",
    "    \"q000_segmentation.npy\",\n",
    "    \"q001_segmentation.npy\",\n",
    "    \"q002_segmentation.npy\",\n",
    "    \"q003_segmentation.npy\",\n",
    "    \"q004_segmentation.npy\",\n",
    "    \"q005_segmentation.npy\",\n",
    "    \"q006_segmentation.npy\",\n",
    "    \"q007_segmentation.npy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These subfolders will be created/populated in the data folder\n",
    "\n",
    "data_arrays_folder    = \"DataArrays\"\n",
    "notebooks_save_folder = \"SavedNotebooks\"\n",
    "results_save_folder   = \"SavedResults\"\n",
    "models_save_folder    = \"SavedModels\"\n",
    "val_data_folder       = \"PredictionsValidation\"\n",
    "\n",
    "data_arrays_fullpath = os.path.join(local_data_folder, data_arrays_folder)\n",
    "notebooks_save_fullpath = os.path.join(local_data_folder, notebooks_save_folder)\n",
    "results_save_fullpath = os.path.join(local_data_folder, results_save_folder)\n",
    "models_save_fullpath = os.path.join(local_data_folder, models_save_folder)\n",
    "val_data_fullpath = os.path.join(local_data_folder, val_data_folder)\n",
    "\n",
    "if not os.path.exists(data_arrays_fullpath):\n",
    "    os.makedirs(data_arrays_fullpath)\n",
    "    print(\"Created folder: {}\".format(data_arrays_fullpath))\n",
    "\n",
    "if not os.path.exists(notebooks_save_fullpath):\n",
    "    os.makedirs(notebooks_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(notebooks_save_fullpath))\n",
    "\n",
    "if not os.path.exists(results_save_fullpath):\n",
    "    os.makedirs(results_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(results_save_fullpath))\n",
    "\n",
    "if not os.path.exists(models_save_fullpath):\n",
    "    os.makedirs(models_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(models_save_fullpath))\n",
    "\n",
    "if not os.path.exists(val_data_fullpath):\n",
    "    os.makedirs(val_data_fullpath)\n",
    "    print(\"Created folder: {}\".format(val_data_fullpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training files ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a86732808aa4e8e94cc61fb89ff8c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total download time: 0:00:00.015228\n"
     ]
    }
   ],
   "source": [
    "# Download data from Girder\n",
    "\n",
    "time_download_start = datetime.datetime.now()\n",
    "\n",
    "print(\"Downloading training files ...\")\n",
    "\n",
    "# Setting up number of validation rounds\n",
    "\n",
    "n_files = len(training_ultrasound_ids)\n",
    "if limit_validation_rounds > 0:\n",
    "    num_validation_rounds = min(n_files, limit_validation_rounds)\n",
    "else:\n",
    "    num_validation_rounds = n_files\n",
    "\n",
    "# Preparing progress bar\n",
    "\n",
    "f = IntProgress(min=0, max=n_files*2)\n",
    "display(f)\n",
    "\n",
    "# Downloading files\n",
    "\n",
    "gclient = girder_client.GirderClient(apiUrl=girder_api_url)\n",
    "\n",
    "for i in range(n_files):\n",
    "    ultrasound_fullname = os.path.join(data_arrays_fullpath, training_ultrasound_filenames[i])\n",
    "    if not os.path.exists(ultrasound_fullname) or overwrite_existing_data_files:\n",
    "        print(\"Downloading {}...\".format(ultrasound_fullname))\n",
    "        gclient.downloadFile(training_ultrasound_ids[i], ultrasound_fullname)\n",
    "    f.value = i * 2 + 1\n",
    "    \n",
    "    segmentation_fullname = os.path.join(data_arrays_fullpath, training_segmentation_filenames[i])\n",
    "    if not os.path.exists(segmentation_fullname) or overwrite_existing_data_files:\n",
    "        print(\"Downloading {}...\".format(segmentation_fullname))\n",
    "        gclient.downloadFile(training_segmentation_ids[i], segmentation_fullname)\n",
    "    f.value = i * 2 + 2\n",
    "\n",
    "time_download_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal download time: {}\".format(time_download_stop - time_download_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060c084a8a564401809fb012df95c4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time to load from files: 0:00:00.154389\n"
     ]
    }
   ],
   "source": [
    "# Read data into torch tensors in channel-first format, dtype float\n",
    "\n",
    "ultrasound_tensors = []\n",
    "segmentation_tensors = []\n",
    "\n",
    "f = IntProgress(min=0, max=n_files * 2)\n",
    "display(f)\n",
    "\n",
    "time_start = datetime.datetime.now()\n",
    "\n",
    "for i in range(n_files):\n",
    "    ultrasound_fullname = os.path.join(data_arrays_fullpath, training_ultrasound_filenames[i])\n",
    "    segmentation_fullname = os.path.join(data_arrays_fullpath, training_segmentation_filenames[i])\n",
    "\n",
    "    ultrasound_data = np.load(ultrasound_fullname)\n",
    "    ultrasound_data = torch.Tensor(ultrasound_data).permute(0,3,1,2).float()\n",
    "    f.value = i * 2 + 1\n",
    "    \n",
    "    segmentation_data = np.load(segmentation_fullname)\n",
    "    segmentation_data = torch.Tensor(segmentation_data).long().permute(0,3,1,2)\n",
    "\n",
    "    f.value = i * 2 + 2\n",
    "    \n",
    "    ultrasound_tensors.append(ultrasound_data)\n",
    "    segmentation_tensors.append(segmentation_data)\n",
    "\n",
    "time_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal time to load from files: {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "# Use cuda GPU if available\n",
    "\n",
    "device_name = \" \"\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "    \n",
    "print('Using device:', device_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp for saved files: 2021-08-05_19-38-26\n",
      "\n",
      "Training parameters\n",
      "Number of epochs:    500\n",
      "Step size maximum:   0.025\n",
      "Step size decay:     0.9876477074806245\n",
      "Batch size:          128\n",
      "Regularization rate: 0.001\n",
      "\n",
      "Saving validation predictions in: /home/nick/dev/SaggitalSpineSegmentation_Data/PredictionsValidation\n",
      "Saving models in:                 /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels\n",
      "\n",
      "*** Leave-one-out round # 0\n",
      "\n",
      "Training on 2767 images, validating on 523 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cf824804f74a9499c55c6823d0905d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.04038921442331593\n",
      "  val_dice:      0.9919690370559693\n",
      "  Training time: 0:23:47.956401\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-0_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:23:48.143896\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 1\n",
      "\n",
      "Training on 2935 images, validating on 355 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b0551fe1884acda1967e36cb998321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.06233633270775768\n",
      "  val_dice:      0.9970506429672241\n",
      "  Training time: 0:23:54.552124\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-1_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:23:54.687787\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 2\n",
      "\n",
      "Training on 2813 images, validating on 477 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd7ef5fcb72424e8fc860982cf3c9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.03880398462778367\n",
      "  val_dice:      0.9939527809619904\n",
      "  Training time: 0:23:50.801667\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-2_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:23:50.975367\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 3\n",
      "\n",
      "Training on 2837 images, validating on 453 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5446635d0524cc981894adc550b61cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.04968169088132071\n",
      "  val_dice:      0.9925963282585144\n",
      "  Training time: 0:23:58.301991\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-3_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:23:58.470822\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 4\n",
      "\n",
      "Training on 3001 images, validating on 289 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42fb1de5b79400f8f65cfa65e68cf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.034604349170382874\n",
      "  val_dice:      0.9937962492307028\n",
      "  Training time: 0:24:08.222300\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-4_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:24:08.331600\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 5\n",
      "\n",
      "Training on 2903 images, validating on 387 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fd04ee48d448e098a6938e3329bf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.043595006915642\n",
      "  val_dice:      0.9906564205884933\n",
      "  Training time: 0:25:30.609656\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-5_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:25:30.761608\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 6\n",
      "\n",
      "Training on 2930 images, validating on 360 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0972aa5606648bf867572d269403622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.0523686569597986\n",
      "  val_dice:      0.9878193736076355\n",
      "  Training time: 0:26:17.636359\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-6_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:26:17.782675\n",
      "\n",
      "\n",
      "*** Leave-one-out round # 7\n",
      "\n",
      "Training on 2844 images, validating on 446 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beda18b886b347e7a877a12987b434be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics at the end of training\n",
      "  val loss:      0.0490972580296309\n",
      "  val_dice:      0.9920045435428619\n",
      "  Training time: 0:26:34.270043\n",
      "/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-7_2021-08-05_19-38-26.msd\n",
      "\n",
      "Total round time:  0:26:34.444891\n",
      "\n",
      "\n",
      "Total training time:   3:18:10.267234\n"
     ]
    }
   ],
   "source": [
    "# This is where all the training gets done\n",
    "\n",
    "# Print training parameters, to archive them together with the notebook output.\n",
    "\n",
    "time_sequence_start = datetime.datetime.now()\n",
    "\n",
    "print(\"Timestamp for saved files: {}\".format(save_timestamp))\n",
    "print(\"\\nTraining parameters\")\n",
    "print(\"Number of epochs:    {}\".format(num_epochs))\n",
    "print(\"Step size maximum:   {}\".format(max_learning_rate))\n",
    "print(\"Step size decay:     {}\".format(learning_rate_decay))\n",
    "print(\"Batch size:          {}\".format(batch_size))\n",
    "print(\"Regularization rate: {}\".format(regularization_rate))\n",
    "print(\"\")\n",
    "print(\"Saving validation predictions in: {}\".format(val_data_fullpath))\n",
    "print(\"Saving models in:                 {}\".format(models_save_fullpath))\n",
    "\n",
    "# ROC data will be saved in these containers\n",
    "\n",
    "val_best_metrics    = dict()\n",
    "val_fuzzy_metrics   = dict()\n",
    "val_aurocs          = np.zeros(num_validation_rounds)\n",
    "val_best_thresholds = np.zeros(num_validation_rounds)\n",
    "\n",
    "# Initialize metrics\n",
    "\n",
    "train_loss = 0.0\n",
    "val_loss = 0.0\n",
    "val_dice = 0.0\n",
    "\n",
    "# Perform validation rounds\n",
    "\n",
    "for i in range(num_validation_rounds):\n",
    "    \n",
    "    f = IntProgress(min=0, max=num_epochs)\n",
    "     \n",
    "    # Set Up TensorBoard\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "#   Prepare data arrays\n",
    "#   leave out ultrasound_arrays[i]\n",
    "    \n",
    "    train_ultrasound_data = torch.zeros(\n",
    "        [0, ultrasound_tensors[0].shape[1], ultrasound_tensors[0].shape[2], ultrasound_tensors[0].shape[3]]).float()\n",
    "    train_segmentation_data = torch.zeros(\n",
    "        [0, ultrasound_tensors[0].shape[1], ultrasound_tensors[0].shape[2], ultrasound_tensors[0].shape[3]]).long()\n",
    "    \n",
    "    val_ultrasound_data = ultrasound_tensors[i]\n",
    "    val_segmentation_data = segmentation_tensors[i]\n",
    "    val_ultrasound_filename = training_ultrasound_filenames[i]\n",
    "    \n",
    "    for train_index in range(n_files):\n",
    "        if train_index != i:\n",
    "            train_ultrasound_data = torch.cat((train_ultrasound_data, ultrasound_tensors[train_index]))\n",
    "            train_segmentation_data = torch.cat((train_segmentation_data, segmentation_tensors[train_index]))\n",
    "    \n",
    "    n_train = train_ultrasound_data.size(0)\n",
    "    n_val = val_ultrasound_data.size(0)\n",
    "    \n",
    "    print(\"\\n*** Leave-one-out round # {}\".format(i))\n",
    "    print(\"\\nTraining on {} images, validating on {} images...\".format(n_train, n_val))\n",
    "    \n",
    "    display(f)\n",
    "    \n",
    "    # Create and train model\n",
    "\n",
    "    model = UNet(128,num_classes).to(device).train()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=max_learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optim, gamma=learning_rate_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(WCE_weights).float()).to(device)\n",
    "    dice_metric = torchmetrics.F1(num_classes=num_classes, mdmc_average='global').to(device).eval()\n",
    "    softmax = torch.nn.Softmax(dim=1).to(device)\n",
    "    \n",
    "    # PyTorch Datasets and DataLoaders\n",
    "    \n",
    "    training_dataset = DataAugmentor(train_ultrasound_data,\n",
    "                                     train_segmentation_data,\n",
    "                                     image_dimensions=(ultrasound_size, ultrasound_size),\n",
    "                                     max_rotation_angle=max_rotation_angle,\n",
    "                                     max_shift_factor=max_shift_factor,\n",
    "                                     min_zoom_factor=min_zoom_factor,\n",
    "                                     max_zoom_factor=max_zoom_factor)\n",
    "    training_generator = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    val_dataset = DataAugmentor(val_ultrasound_data,\n",
    "                                val_segmentation_data,\n",
    "                                image_dimensions=(ultrasound_size, ultrasound_size),\n",
    "                                max_rotation_angle=max_rotation_angle,\n",
    "                                max_shift_factor=max_shift_factor,\n",
    "                                min_zoom_factor=min_zoom_factor,\n",
    "                                max_zoom_factor=max_zoom_factor)\n",
    "    val_generator = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "        \n",
    "    training_time_start = datetime.datetime.now()\n",
    "    \n",
    "    # training loop for this validation split\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        \n",
    "        # training\n",
    "        model.train()\n",
    "        num = 0\n",
    "        for batch, target in training_generator:\n",
    "            num += 1\n",
    "            optim.zero_grad()\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "            pred = model(batch).to(device)\n",
    "            loss = criterion(pred, target.squeeze(1))\n",
    "            loss += regularization_rate * sum(x.abs().sum()for k, x in model.named_parameters() if k.endswith('conv.bias')) * 1e-5\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * batch.size(0)\n",
    "            optim.step()\n",
    "            \n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        train_loss = train_loss / training_dataset.__len__()\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        num = 0\n",
    "        for batch, target in val_generator:\n",
    "            num += 1\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch).to(device)\n",
    "                loss = criterion(pred, target.squeeze(1))\n",
    "                val_loss += loss.item() * batch.size(0)\n",
    "                pred_probmap = softmax(pred)\n",
    "                dice = dice_metric(pred_probmap, target)\n",
    "                val_dice += dice.item()\n",
    "        \n",
    "        if epoch==1:\n",
    "            writer.add_graph(model, batch)\n",
    "            \n",
    "        val_loss = val_loss / val_dataset.__len__()\n",
    "        val_dice = val_dice / num\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "        writer.add_scalar('Dice/validation', val_dice, epoch)\n",
    "        writer.add_scalar('Meta/learning_rate', lr_scheduler.get_last_lr()[-1], epoch)\n",
    "        \n",
    "        f.value = epoch\n",
    "        \n",
    "\n",
    "    training_time_stop = datetime.datetime.now()\n",
    "    \n",
    "    # Print training log\n",
    "    \n",
    "    print(\"\\nMetrics at the end of training\")\n",
    "    print(\"  val loss:      {}\".format(val_loss))\n",
    "    print(\"  val_dice:      {}\".format(val_dice))\n",
    "    print(\"  Training time: {}\".format(training_time_stop-training_time_start))\n",
    "    \n",
    "    # TODO Plot training loss and metrics\n",
    "    \n",
    "    # Predict on validation data\n",
    "    \n",
    "    y_pred_val  = model(val_ultrasound_data.to(device))\n",
    "    \n",
    "    # Saving predictions for further evaluation\n",
    "    \n",
    "    filename_noext, extension = os.path.splitext(val_ultrasound_filename)\n",
    "    val_prediction_filename = save_timestamp + \"_prediction_\" + filename_noext + \".npy\"\n",
    "    val_prediction_fullname = os.path.join(val_data_fullpath, val_prediction_filename)\n",
    "    torch.save(y_pred_val, val_prediction_fullname)\n",
    "    \n",
    "    # Archive trained model with unique filename based on notebook name and timestamp\n",
    "    \n",
    "    model_file_name = this_notebook_name + \"_model-\" + str(i) + \"_\" + save_timestamp + \".msd\"\n",
    "    model_fullname = os.path.join(models_save_fullpath, model_file_name)\n",
    "    print(model_fullname)\n",
    "    torch.save(model.state_dict(), model_fullname)\n",
    "    \n",
    "    # Validation results\n",
    "     \n",
    "#     vali_metrics_dicts, vali_best_threshold_index, vali_area = evaluation_metrics.compute_roc(\n",
    "#         roc_thresholds, y_pred_val, val_segmentation_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "#     val_fuzzy_metrics[i] = evaluation_metrics.compute_evaluation_metrics(\n",
    "#         y_pred_val, val_segmentation_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "#     val_best_metrics[i]    = vali_metrics_dicts[vali_best_threshold_index]\n",
    "#     val_aurocs[i]          = vali_area\n",
    "#     val_best_thresholds[i] = roc_thresholds[vali_best_threshold_index]\n",
    "    \n",
    "    # Printing total time of this validation round\n",
    "    \n",
    "    print(\"\\nTotal round time:  {}\".format(datetime.datetime.now() - training_time_start))\n",
    "    print(\"\")\n",
    "    \n",
    "    # just do one validation split\n",
    "#     break\n",
    "\n",
    "\n",
    "time_sequence_stop = datetime.datetime.now()\n",
    "\n",
    "print(\"\\nTotal training time:   {}\".format(time_sequence_stop - time_sequence_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require([\"base/js/namespace\"],function(Jupyter) {\n",
       "    Jupyter.notebook.save_checkpoint();\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save notebook so all output is archived by the next cell\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedNotebooks/PyTorchSagittalSpineSegmentationStudy_2021-08-05_19-38-26.html\n"
     ]
    }
   ],
   "source": [
    "# Export HTML copy of this notebook\n",
    "\n",
    "notebook_file_name = this_notebook_name + \"_\" + save_timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebooks_save_fullpath, notebook_file_name)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \" + this_notebook_name + \" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([446, 128, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sm = torch.nn.functional.softmax(y_pred_val, dim=1)\n",
    "pred_sm = pred_sm[:,1,:,:]\n",
    "pred_sm.size()\n",
    "# torch.nn.functional.softmax(y_pred_val, dim=1)[:,1,:,:].to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sm_np = np.array(pred_sm.to('cpu').detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_np = torch.squeeze(val_segmentation_data, dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = int(523*np.random.rand())\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(20,15)\n",
    "axs[0].imshow(gt_np[index])\n",
    "axs[1].imshow(pred_sm_np[index])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah#blah#blah\n",
    "#blah#blah\n",
    "#blah#blah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
