{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe9aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropd,\n",
    "    RandZoomd,\n",
    "    Resized,\n",
    "    ScaleIntensityRanged,\n",
    "    SpatialCrop,\n",
    "    SpatialCropd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itk\n",
    "\n",
    "import site\n",
    "site.addsitedir('../../../ARGUS')\n",
    "from ARGUSUtils_Transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d715cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "49 8 5\n",
      "51 5 6\n",
      "50 6 6\n",
      "50 6 6\n",
      "51 6 5\n",
      "51 5 6\n",
      "50 6 6\n",
      "49 6 7\n",
      "48 7 7\n",
      "47 7 8\n"
     ]
    }
   ],
   "source": [
    "img1_dir = \"../../../Data/VFoldData/BAMC-PTX*Sliding-Annotations-Linear/\"\n",
    "\n",
    "all_images = sorted(glob(os.path.join(img1_dir, '*_?????.mha')))\n",
    "all_labels = sorted(glob(os.path.join(img1_dir, '*.interpolated-overlay-3class.mha')))\n",
    "\n",
    "num_classes = 3\n",
    "num_workers_tr = 8\n",
    "batch_size_tr = 8\n",
    "num_workers_vl = 8\n",
    "batch_size_vl = 2\n",
    "\n",
    "num_slices = 16\n",
    "size_x = 240\n",
    "size_y = 240\n",
    "\n",
    "model_filename_base = \"BAMC_PTX_3DUNet-3Class.best_model.vfold\"\n",
    "\n",
    "num_images = len(all_images)\n",
    "print(num_images)\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "ns_prefix = ['025ns','026ns','027ns','035ns','048ns','055ns','117ns',\n",
    "             '135ns','193ns','210ns','215ns','218ns','219ns','221ns','247ns']\n",
    "s_prefix = ['004s','019s','030s','034s','037s','043s','065s','081s',\n",
    "            '206s','208s','211s','212s','224s','228s','236s','237s']\n",
    "\n",
    "fold_prefix_list = []\n",
    "ns_count = 0\n",
    "s_count = 0\n",
    "for i in range(num_folds):\n",
    "    if i%2 == 0:\n",
    "        num_ns = 2\n",
    "        num_s = 1\n",
    "        if i > num_folds-3:\n",
    "            num_s = 2\n",
    "    else:\n",
    "        num_ns = 1\n",
    "        num_s = 2\n",
    "    f = []\n",
    "    for ns in range(num_ns):\n",
    "        f.append([ns_prefix[ns_count+ns]])\n",
    "    ns_count += num_ns\n",
    "    for s in range(num_s):\n",
    "        f.append([s_prefix[s_count+s]])\n",
    "    s_count += num_s\n",
    "    fold_prefix_list.append(f)\n",
    "        \n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "for i in range(num_folds):\n",
    "    tr_folds = []\n",
    "    for f in range(i,i+num_folds-2):\n",
    "        tr_folds.append(fold_prefix_list[f%num_folds])\n",
    "    tr_folds = list(np.concatenate(tr_folds).flat)\n",
    "    va_folds = list(np.concatenate(fold_prefix_list[(i+num_folds-2) % num_folds]).flat)\n",
    "    te_folds = list(np.concatenate(fold_prefix_list[(i+num_folds-1) % num_folds]).flat)\n",
    "    train_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in tr_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in tr_folds)])\n",
    "            ]\n",
    "        )\n",
    "    val_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in va_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in va_folds)])\n",
    "            ]\n",
    "        )\n",
    "    test_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in te_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in te_folds)])\n",
    "            ]\n",
    "        )\n",
    "    print(len(train_files[i]),len(val_files[i]),len(test_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e813dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = itk.imread(train_files[0][0][\"image\"])\n",
    "arr = itk.GetArrayFromImage(img)\n",
    "imgshape = list(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5bc7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],\n",
    "            a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0),\n",
    "        ARGUS_RandSpatialCropSlicesd(num_slices=num_slices,\n",
    "            axis=2,\n",
    "            keys=['image', 'label']),\n",
    "        RandFlipd(prob=0.5, \n",
    "            spatial_axis=2,\n",
    "            keys=['image', 'label']),\n",
    "        RandFlipd(prob=0.5, \n",
    "            spatial_axis=0,\n",
    "            keys=['image', 'label']),\n",
    "        RandZoomd(prob=0.5, \n",
    "            min_zoom=1.0,\n",
    "            max_zoom=1.2,\n",
    "            keep_size=True,\n",
    "            mode=['trilinear', 'nearest'],\n",
    "            keys=['image', 'label']),\n",
    "        RandSpatialCropd(\n",
    "            roi_size=(size_x, size_y, num_slices),\n",
    "            random_size=False,\n",
    "            keys=['image', 'label']),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],\n",
    "            a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0),\n",
    "        ARGUS_RandSpatialCropSlicesd(num_slices=num_slices,\n",
    "            axis=2,\n",
    "            center_slice=30,\n",
    "            keys=['image', 'label']),\n",
    "        SpatialCropd(\n",
    "            roi_start=((imgshape[0]-size_x)//2, (imgshape[1]-size_y)//2, 0),\n",
    "            roi_end=((imgshape[0]-size_x)//2+size_x, (imgshape[1]-size_y)//2+size_y, num_slices),\n",
    "            keys=['image', 'label']),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23455824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████| 49/49 [00:00<00:00, 81.71it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 51/51 [00:00<00:00, 80.51it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 50/50 [00:00<00:00, 81.51it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 50/50 [00:00<00:00, 80.41it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 51/51 [00:00<00:00, 76.76it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 51/51 [00:02<00:00, 22.89it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 50/50 [00:03<00:00, 14.60it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 49/49 [00:03<00:00, 12.26it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 48/48 [00:03<00:00, 13.43it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 47/47 [00:04<00:00, 10.55it/s]\n",
      "Loading dataset: 100%|███████████████████████████| 8/8 [00:00<00:00, 153.87it/s]\n",
      "Loading dataset: 100%|████████████████████████████| 5/5 [00:00<00:00, 91.92it/s]\n",
      "Loading dataset: 100%|███████████████████████████| 6/6 [00:00<00:00, 135.80it/s]\n",
      "Loading dataset: 100%|███████████████████████████| 6/6 [00:00<00:00, 111.70it/s]\n",
      "Loading dataset: 100%|███████████████████████████| 6/6 [00:00<00:00, 101.23it/s]\n",
      "Loading dataset: 100%|████████████████████████████| 5/5 [00:00<00:00, 55.62it/s]\n",
      "Loading dataset: 100%|████████████████████████████| 6/6 [00:00<00:00, 29.86it/s]\n",
      "Loading dataset: 100%|████████████████████████████| 6/6 [00:00<00:00, 13.35it/s]\n",
      "Loading dataset: 100%|████████████████████████████| 7/7 [00:00<00:00, 23.71it/s]\n",
      "Loading dataset: 100%|████████████████████████████| 7/7 [00:00<00:00, 11.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = [CacheDataset(data=train_files[i], transform=train_transforms,cache_rate=1.0, num_workers=num_workers_tr)\n",
    "            for i in range(num_folds)]\n",
    "train_loader = [DataLoader(train_ds[i], batch_size=batch_size_tr, shuffle=True, num_workers=num_workers_tr) \n",
    "                for i in range(num_folds)]\n",
    "\n",
    "val_ds = [CacheDataset(data=val_files[i], transform=val_transforms, cache_rate=1.0, num_workers=num_workers_vl)\n",
    "          for i in range(num_folds)]\n",
    "val_loader = [DataLoader(val_ds[i], batch_size=batch_size_vl, num_workers=num_workers_vl)\n",
    "              for i in range(num_folds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ae6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def vfold_train(vfold_num, train_loader, val_loader):\n",
    "    model = UNet(\n",
    "        dimensions=3,\n",
    "        in_channels=1,\n",
    "        out_channels=num_classes,\n",
    "        channels=(16, 32, 64, 128),\n",
    "        strides=(2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        norm=Norm.BATCH,\n",
    "    ).to(device)\n",
    "    loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "\n",
    "    max_epochs = 500\n",
    "    val_interval = 2\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=True, num_classes=num_classes)])\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=True, num_classes=num_classes)])\n",
    "\n",
    "    root_dir = \".\"\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"{vfold_num}: epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(\n",
    "                f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"train_loss: {loss.item():.4f}\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"{vfold_num} epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    roi_size = (size_x, size_y, num_slices)\n",
    "                    sw_batch_size = batch_size_vl\n",
    "                    val_outputs = sliding_window_inference(\n",
    "                        val_inputs, roi_size, sw_batch_size, model)\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    # compute metric for current iteration\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                # aggregate the final mean dice result\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                # reset the status for next validation round\n",
    "                dice_metric.reset()\n",
    "\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), os.path.join(\n",
    "                        root_dir, model_filename_base+'_'+str(vfold_num)+'.pth'))\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                    f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                    f\"at epoch: {best_metric_epoch}\"\n",
    "                )\n",
    "\n",
    "    np.save(model_filename_base+\"_loss_\"+str(vfold_num)+\".npy\", epoch_loss_values)\n",
    "    np.save(model_filename_base+\"_val_dice_\"+str(vfold_num)+\".npy\", metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9b78e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "2: epoch 1/500\n",
      "1/1, train_loss: 0.7795\n",
      "2/1, train_loss: 0.7835\n",
      "3/1, train_loss: 0.7898\n",
      "4/1, train_loss: 0.7807\n",
      "5/1, train_loss: 0.7897\n",
      "6/1, train_loss: 0.7716\n",
      "7/1, train_loss: 0.7737\n",
      "2 epoch 1 average loss: 0.7812\n",
      "----------\n",
      "2: epoch 2/500\n",
      "1/1, train_loss: 0.7725\n",
      "2/1, train_loss: 0.7761\n",
      "3/1, train_loss: 0.7597\n",
      "4/1, train_loss: 0.7629\n",
      "5/1, train_loss: 0.7669\n",
      "6/1, train_loss: 0.7657\n",
      "7/1, train_loss: 0.7634\n",
      "2 epoch 2 average loss: 0.7668\n",
      "saved new best metric model\n",
      "current epoch: 2 current mean dice: 0.0144\n",
      "best mean dice: 0.0144 at epoch: 2\n",
      "----------\n",
      "2: epoch 3/500\n",
      "1/1, train_loss: 0.7735\n",
      "2/1, train_loss: 0.7531\n",
      "3/1, train_loss: 0.7654\n",
      "4/1, train_loss: 0.7543\n",
      "5/1, train_loss: 0.7536\n",
      "6/1, train_loss: 0.7471\n",
      "7/1, train_loss: 0.7790\n",
      "2 epoch 3 average loss: 0.7609\n",
      "----------\n",
      "2: epoch 4/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f1d22e2a940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/local/KHQ/stephen.aylward/anaconda3/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function _releaseLock at 0x7f1d22e2a940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/local/KHQ/stephen.aylward/anaconda3/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 82462, 82534, 82606, 82678) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76146/218774476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvfold_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# In[ ]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_76146/3246593711.py\u001b[0m in \u001b[0;36mvfold_train\u001b[0;34m(vfold_num, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             inputs, labels = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 82462, 82534, 82606, 82678) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for i in range(2,num_folds):\n",
    "    vfold_train(i, train_loader[i], val_loader[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874909d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
