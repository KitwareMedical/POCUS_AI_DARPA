{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c893f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    LabelToMaskd,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropd,\n",
    "    RandZoomd,\n",
    "    RepeatChanneld,\n",
    "    Resized,\n",
    "    ScaleIntensityRanged,\n",
    "    SpatialCrop,\n",
    "    SpatialCropd,\n",
    "    SplitChanneld,\n",
    "    ThresholdIntensityd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itk\n",
    "\n",
    "import sys\n",
    "\n",
    "import site\n",
    "site.addsitedir('../../ARGUS')\n",
    "from ARGUSUtils_Transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81219e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_dir = \"../../Data/VFoldData/ROIData/\"\n",
    "\n",
    "all_images1 = sorted(glob(os.path.join(img1_dir, '*.roi.nii.gz')))\n",
    "all_labels = sorted(glob(os.path.join(img1_dir, '*.roi.extruded-overlay-S.nii.gz')))\n",
    "\n",
    "for i in range(len(all_images1)):\n",
    "    img1 = itk.imread(all_images1[i])\n",
    "    lbl = itk.imread(all_labels[i])\n",
    "    if img1.shape != lbl.shape:\n",
    "        print( lbl.shape, img1.shape)\n",
    "        print(\"Error1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057c56b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "96 6 8\n",
      "92 8 10\n",
      "93 10 7\n",
      "97 7 6\n",
      "96 6 8\n",
      "98 8 4\n",
      "100 4 6\n",
      "97 6 7\n",
      "98 7 5\n",
      "92 5 13\n",
      "92 13 5\n",
      "98 5 7\n",
      "93 7 10\n",
      "92 10 8\n",
      "96 8 6\n"
     ]
    }
   ],
   "source": [
    "img1_dir = \"../../Data/VFoldData/ROIData/\"\n",
    "\n",
    "all_images = sorted(glob(os.path.join(img1_dir, '*.roi.nii.gz')))\n",
    "all_labels = sorted(glob(os.path.join(img1_dir, '*.roi.extruded-overlay-S.nii.gz')))\n",
    "\n",
    "gpu_device = 0\n",
    "num_gpu_devices = 1\n",
    "#if len(sys.argv)==3:\n",
    "    #gpu_device = int(sys.argv[1])\n",
    "    #num_gpu_devices = int(sys.argv[2])\n",
    "    \n",
    "num_classes = 2\n",
    "\n",
    "net_channels = (24, 32, 64)\n",
    "net_strides = (2, 2)\n",
    "\n",
    "num_folds = 15\n",
    "\n",
    "num_slices = 48\n",
    "size_x = 128\n",
    "size_y = 224\n",
    "\n",
    "roi_size = (size_x,size_y,num_slices)\n",
    "\n",
    "num_workers_tr = 1\n",
    "batch_size_tr = 24\n",
    "num_workers_vl = 1\n",
    "batch_size_vl = 4\n",
    "\n",
    "\n",
    "model_filename_base = \"./results/BAMC_PTX_ROI_3DUNet-Extruded-S.best_model.vfold\"\n",
    "\n",
    "num_images = len(all_images)\n",
    "print(num_images)\n",
    "\n",
    "ns_prefix = ['025ns','026ns','027ns','035ns','048ns','055ns','117ns',\n",
    "             '135ns','193ns','210ns','215ns','218ns','219ns','221ns','247ns']\n",
    "s_prefix = ['004s','019s','030s','034s','037s','043s','065s','081s',\n",
    "            '206s','208s','211s','212s','224s','228s','236s','237s']\n",
    "\n",
    "fold_prefix_list = []\n",
    "ns_count = 0\n",
    "s_count = 0\n",
    "for i in range(num_folds):\n",
    "    if i%2 == 0:\n",
    "        num_ns = 1\n",
    "        num_s = 1\n",
    "        if i > num_folds-3:\n",
    "            num_s = 2\n",
    "    else:\n",
    "        num_ns = 1\n",
    "        num_s = 1\n",
    "    f = []\n",
    "    for ns in range(num_ns):\n",
    "        f.append([ns_prefix[ns_count+ns]])\n",
    "    ns_count += num_ns\n",
    "    for s in range(num_s):\n",
    "        f.append([s_prefix[s_count+s]])\n",
    "    s_count += num_s\n",
    "    fold_prefix_list.append(f)\n",
    "        \n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "for i in range(num_folds):\n",
    "    tr_folds = []\n",
    "    for f in range(i,i+num_folds-2):\n",
    "        tr_folds.append(fold_prefix_list[f%num_folds])\n",
    "    tr_folds = list(np.concatenate(tr_folds).flat)\n",
    "    va_folds = list(np.concatenate(fold_prefix_list[(i+num_folds-2) % num_folds]).flat)\n",
    "    te_folds = list(np.concatenate(fold_prefix_list[(i+num_folds-1) % num_folds]).flat)\n",
    "    train_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in tr_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in tr_folds)])\n",
    "            ]\n",
    "        )\n",
    "    val_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in va_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in va_folds)])\n",
    "            ]\n",
    "        )\n",
    "    test_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in te_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in te_folds)])\n",
    "            ]\n",
    "        )\n",
    "    print(len(train_files[i]),len(val_files[i]),len(test_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8b0a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],\n",
    "            a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0),\n",
    "        ARGUS_RandSpatialCropSlicesd(\n",
    "            num_slices=num_slices,\n",
    "            axis=3,\n",
    "            keys=['image', 'label']),\n",
    "        RandFlipd(prob=0.5, \n",
    "            spatial_axis=2,\n",
    "            keys=['image', 'label']),\n",
    "        RandFlipd(prob=0.5, \n",
    "            spatial_axis=0,\n",
    "            keys=['image', 'label']),\n",
    "        RandZoomd(prob=0.5, \n",
    "            min_zoom=1.0,\n",
    "            max_zoom=1.2,\n",
    "            keys=['image', 'label'],\n",
    "            keep_size=True,\n",
    "            mode=['trilinear', 'nearest']),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],\n",
    "            a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0),\n",
    "        ARGUS_RandSpatialCropSlicesd(\n",
    "            num_slices=num_slices,\n",
    "            axis=3,\n",
    "            center_slice=30,\n",
    "            keys=['image', 'label']),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3385d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 224, 48)\n"
     ]
    }
   ],
   "source": [
    "img = itk.imread(train_files[0][0][\"image\"])\n",
    "arr = itk.GetArrayFromImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bafe72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████| 96/96 [00:03<00:00, 31.52it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 92/92 [00:02<00:00, 34.59it/s]\n",
      "Loading dataset: 100%|██████████████████████████| 93/93 [00:02<00:00, 32.50it/s]\n",
      "Loading dataset:  35%|█████████                 | 34/97 [00:01<00:01, 32.13it/s]"
     ]
    }
   ],
   "source": [
    "train_ds = [CacheDataset(data=train_files[i], transform=train_transforms,cache_rate=1.0, num_workers=None)\n",
    "            for i in range(num_folds)]\n",
    "train_loader = [DataLoader(train_ds[i], batch_size=batch_size_tr, shuffle=True, num_workers=num_workers_tr) \n",
    "                for i in range(num_folds)]\n",
    "\n",
    "val_ds = [CacheDataset(data=val_files[i], transform=val_transforms, cache_rate=1.0, num_workers=None)\n",
    "          for i in range(num_folds)]\n",
    "val_loader = [DataLoader(val_ds[i], batch_size=batch_size_vl, num_workers=num_workers_vl)\n",
    "              for i in range(num_folds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f616ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #len(sys.argv)>0:\n",
    "    imgnum = 0\n",
    "    check_data = first(val_loader[0])\n",
    "    print(val_files[0][imgnum])\n",
    "    image, label = (check_data[\"image\"][imgnum][0], check_data[\"label\"][imgnum][0])\n",
    "    print(check_data[\"image\"].shape)\n",
    "    print(image.shape)\n",
    "    print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image[:, :, 2], cmap=\"gray\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(label[:, :, 2])\n",
    "    plt.show()\n",
    "    print(label.min(), label.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fe1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:\"+str(gpu_device))\n",
    "\n",
    "def vfold_train(vfold_num, train_loader, val_loader):\n",
    "    model = UNet(\n",
    "        dimensions=3,\n",
    "        in_channels=1,\n",
    "        out_channels=num_classes,\n",
    "        channels=net_channels,\n",
    "        strides=net_strides,\n",
    "        num_res_units=2,\n",
    "        norm=Norm.BATCH,\n",
    "    ).to(device)\n",
    "    loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "\n",
    "    max_epochs = 500\n",
    "    val_interval = 2\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=True, num_classes=num_classes)])\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=True, num_classes=num_classes)])\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"{vfold_num}: epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(\n",
    "                f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"train_loss: {loss.item():.4f}\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"{vfold_num} epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    val_outputs = sliding_window_inference(\n",
    "                        val_inputs, roi_size, batch_size_vl, model)\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    # compute metric for current iteration\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                # aggregate the final mean dice result\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                # reset the status for next validation round\n",
    "                dice_metric.reset()\n",
    "\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), model_filename_base+'_'+str(vfold_num)+'.pth')\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                    f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                    f\"at epoch: {best_metric_epoch}\"\n",
    "                )\n",
    "\n",
    "    np.save(model_filename_base+\"_loss_\"+str(vfold_num)+\".npy\", epoch_loss_values)\n",
    "    np.save(model_filename_base+\"_val_dice_\"+str(vfold_num)+\".npy\", metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b855e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(gpu_device,num_folds,num_gpu_devices):\n",
    "    vfold_train(i, train_loader[i], val_loader[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310061e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
